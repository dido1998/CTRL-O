# @package _global_
defaults:
  - /experiment/projects/scaling/_opt/cosine_schedule
  - /experiment/projects/scaling/_opt/finetuning_control

experiment:
  base_learning_rate: 0.0001
  batch_size_per_gpu: 128
  encoder_layerwise_lr_decay: 0.85
  encoder_lr_factor: 0.125
  encoder_weight_decay: 0.01

trainer:
  gradient_clip_val: 0.1
  max_steps: 300000

losses:
  mse:
    normalize_target: true
