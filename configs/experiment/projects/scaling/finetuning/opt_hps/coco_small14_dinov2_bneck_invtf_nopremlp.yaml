# @package _global_
# DINOSAUR with finetuning of ViT encoder, improved finetuning HPs, and several SA tweaks:
# - No SA pre-MLP
# - Inverted transformer design with gated update
# - Cosine attention
# - Post SA bottleneck
defaults:
  - /experiment/projects/scaling/coco_small14_dinov2_bneck_nopremlp
  - /experiment/projects/scaling/_model/slot_attention/inv_tf
  - /experiment/projects/scaling/_model/slot_attention/gated_update
  - /experiment/projects/scaling/finetuning/_finetuning
  - /experiment/projects/scaling/finetuning/opt_hps/_settings
  - /experiment/projects/scaling/_opt/bottleneck
  - _self_

models:
  target_feature_extractor:
    dynamic_img_size: true
