# @package _global_
# Add masking to ViT feature extractor, use unmasked target feature extractor.
defaults:
  - /experiment/projects/scaling/lang_controllable_coco_small14_dinov2
  - /experiment/projects/scaling/mapping/mlp_mapping
  - /experiment/projects/scaling/_opt/lang_conditioning/mapping_with_probes
  - _self_

models:
  lang_conditioning:
    _target_: routed.ocl.conditioning.LangConditioning
    n_slots: ${experiment.num_slots}
    object_dim: ${experiment.slot_dim}
    lang_conditioning: false
    name_embedding_path: input.name_embedding
    batch_size_path: input.batch_size
    mask_path: input.contrastive_loss_mask

  perceptual_grouping:
      _target_: routed.ocl.perceptual_grouping.ControllableSlotAttentionGrouping
      feature_dim: ${.object_dim}
      object_dim: ${experiment.slot_dim}
      use_projection_bias: false
      positional_embedding:
        _target_: ocl.neural_networks.wrappers.Sequential
        _args_:
          - _target_: ocl.neural_networks.positional_embedding.DummyPositionEmbed
          - _target_: ocl.neural_networks.build_two_layer_mlp
            input_dim: ${experiment.feature_dim}
            output_dim: ${....feature_dim}
            hidden_dim: "${mul: ${experiment.feature_dim}, 2}"
            initial_layer_norm: true
      ff_mlp:
        _target_: ocl.neural_networks.build_two_layer_mlp
        input_dim: ${..object_dim}
        output_dim: ${..object_dim}
        hidden_dim: "${mul: ${..object_dim}, 4}"
        initial_layer_norm: true
        residual: true
      lang_conditioning: false
      feature_path: mapping
      lang_conditioning_path: lang_conditioning

losses:
  category_prediction:
    _target_: routed.ocl.losses.CategoryPredictionLoss
    dim: ${experiment.feature_dim}
    num_classes: 172
    iter_start: 0
    weight: 1.0
    features_path: object_decoder.lang_per_slot_patches
    target_path: input.category_idx
    control_mask_path: input.selected_indices
